"""
LLM Streaming Support Module

This module provides streaming functionality for LLM chat models using
Server-Sent Events (SSE) and LangChain's streaming callback system.

Key Components:
- StreamingForwarderHandler: Callback handler that forwards tokens to an asyncio.Queue
- SSE formatting helpers for event streams
- Async generators for conversation and explanation streaming
"""

import asyncio
import json
import logging
import uuid
from typing import Any, AsyncGenerator, Dict, List, Optional, Union
from dataclasses import dataclass

from langchain_core.callbacks import BaseCallbackHandler
from langchain_core.messages import AIMessage, HumanMessage
from langchain_core.outputs import LLMResult
from langchain_community.chat_message_histories import SQLChatMessageHistory
from fastapi import Request

from app.llm.chains import ConversationChain
from app.llm.models import get_model_factory
from app.llm.prompts import get_chat_prompt_template

logger = logging.getLogger(__name__)


@dataclass
class StreamChunk:
    """Represents a chunk of streamed data"""
    type: str  # 'start', 'token', 'usage', 'end', 'error'
    content: Optional[str] = None
    usage: Optional[Dict[str, int]] = None
    message: Optional[str] = None
    model: Optional[str] = None
    session_id: Optional[str] = None
    meta: Optional[Dict[str, Any]] = None


class StreamingForwarderHandler(BaseCallbackHandler):
    """
    Callback handler that forwards LLM tokens to an asyncio.Queue for streaming.
    
    This handler captures tokens as they're generated by the LLM and puts them
    into a bounded queue that can be consumed by an async generator.
    """
    
    def __init__(self, queue: asyncio.Queue, max_queue_size: int = 100):
        super().__init__()
        self.queue = queue
        self.max_queue_size = max_queue_size
        self._cancelled = False
        
    def on_llm_start(
        self, serialized: Dict[str, Any], prompts: List[str], **kwargs: Any
    ) -> None:
        """Called when LLM starts running."""
        if not self._cancelled:
            try:
                # Extract model name from serialized data
                model_name = serialized.get("name", "unknown")
                chunk = StreamChunk(type="start", model=model_name)
                self.queue.put_nowait(chunk)
            except asyncio.QueueFull:
                logger.warning("Queue full, dropping start event")
    
    def on_llm_new_token(self, token: str, **kwargs: Any) -> None:
        """Called when a new token is generated."""
        if not self._cancelled and token:
            try:
                chunk = StreamChunk(type="token", content=token)
                self.queue.put_nowait(chunk)
            except asyncio.QueueFull:
                logger.warning(f"Queue full, dropping token: {token[:20]}...")
    
    def on_llm_end(self, response: LLMResult, **kwargs: Any) -> None:
        """Called when LLM finishes running."""
        if not self._cancelled:
            try:
                # Extract usage metadata if available
                usage = None
                if hasattr(response, 'llm_output') and response.llm_output:
                    if 'token_usage' in response.llm_output:
                        token_usage = response.llm_output['token_usage']
                        usage = {
                            'input_tokens': token_usage.get('prompt_tokens', 0),
                            'output_tokens': token_usage.get('completion_tokens', 0),
                            'total_tokens': token_usage.get('total_tokens', 0)
                        }
                
                # Check for usage_metadata in generations (new LangChain format)
                if not usage and response.generations:
                    for generation_list in response.generations:
                        for generation in generation_list:
                            if hasattr(generation, 'message') and hasattr(generation.message, 'usage_metadata'):
                                metadata = generation.message.usage_metadata
                                if metadata:
                                    usage = {
                                        'input_tokens': getattr(metadata, 'input_tokens', 0),
                                        'output_tokens': getattr(metadata, 'output_tokens', 0),
                                        'total_tokens': getattr(metadata, 'total_tokens', 0)
                                    }
                                    break
                        if usage:
                            break
                
                if usage:
                    usage_chunk = StreamChunk(type="usage", usage=usage)
                    self.queue.put_nowait(usage_chunk)
                
                end_chunk = StreamChunk(type="end")
                self.queue.put_nowait(end_chunk)
            except asyncio.QueueFull:
                logger.warning("Queue full, dropping end events")
    
    def on_llm_error(self, error: Union[Exception, KeyboardInterrupt], **kwargs: Any) -> None:
        """Called when LLM encounters an error."""
        if not self._cancelled:
            try:
                error_chunk = StreamChunk(type="error", message=str(error))
                self.queue.put_nowait(error_chunk)
                end_chunk = StreamChunk(type="end")
                self.queue.put_nowait(end_chunk)
            except asyncio.QueueFull:
                logger.warning("Queue full, dropping error events")
    
    def cancel(self):
        """Cancel the handler to stop forwarding tokens."""
        self._cancelled = True


def format_sse(event_data: Dict[str, Any]) -> bytes:
    """
    Format data as Server-Sent Events (SSE) format.
    
    Args:
        event_data: Dictionary to be serialized as JSON
        
    Returns:
        Formatted SSE data as bytes
    """
    return f"data: {json.dumps(event_data, ensure_ascii=False)}\n\n".encode("utf-8")


async def stream_conversation(
    session_id: Optional[str],
    user_input: str,
    model_name: Optional[str] = None,
    request: Optional[Request] = None,
    **model_params
) -> AsyncGenerator[bytes, None]:
    """
    Stream a conversation response using Server-Sent Events.
    
    Args:
        session_id: Conversation session ID (auto-generated if None)
        user_input: User's message
        model_name: Optional model override
        request: FastAPI request object for disconnect detection
        **model_params: Additional model parameters
        
    Yields:
        SSE-formatted bytes for each streaming event
    """
    # Generate session ID if not provided
    if not session_id:
        session_id = str(uuid.uuid4())
    
    logger.info(f"Starting conversation stream for session {session_id}")
    
    # Create bounded queue for token forwarding
    token_queue: asyncio.Queue = asyncio.Queue(maxsize=100)
    handler = StreamingForwarderHandler(token_queue)
    
    try:
        # Get model factory and create streaming model
        factory = get_model_factory()
        
        # Create model with streaming enabled and callback handler
        model = factory.create_model(
            model_name or "conversation", 
            streaming=True,
            callbacks=[handler],
            **model_params
        )
        
        if not model:
            error_data = {"event": "error", "message": "Failed to create model"}
            yield format_sse(error_data)
            return
        
        # Load conversation history
        # Use the same sync connection string approach as ConversationChain
        from app.core.config import get_settings
        settings = get_settings()
        url = settings.database_url
        if "+aiosqlite" in url:
            connection_string = url.replace("+aiosqlite", "")
        elif "+asyncpg" in url:
            connection_string = url.replace("+asyncpg", "+psycopg")
        elif "+aiomysql" in url:
            connection_string = url.replace("+aiomysql", "+pymysql")
        else:
            connection_string = url
        
        history = SQLChatMessageHistory(
            session_id=session_id,
            connection_string=connection_string
        )
        
        # Build message list manually for streaming
        messages = []
        
        # Add system message from prompt template
        prompt_template = get_chat_prompt_template("conversation")
        if prompt_template and hasattr(prompt_template, 'messages'):
            for msg_template in prompt_template.messages:
                if hasattr(msg_template, 'format'):
                    formatted = msg_template.format(user_input=user_input, history=[])
                    if msg_template.type == "system":
                        from langchain_core.messages import SystemMessage
                        messages.append(SystemMessage(content=formatted))
        
        # Add conversation history
        history_messages = history.messages
        messages.extend(history_messages)
        
        # Add current user message
        messages.append(HumanMessage(content=user_input))
        
        # Emit start event
        start_data = {
            "event": "start", 
            "model": model_name or "conversation",
            "session_id": session_id
        }
        yield format_sse(start_data)
        
        # Start streaming in background task
        async def stream_model():
            try:
                accumulated_content = ""
                async for chunk in model.astream(messages):
                    if hasattr(chunk, 'content') and chunk.content:
                        accumulated_content += chunk.content
                # Store the accumulated content for history persistence
                handler._accumulated_content = accumulated_content
            except Exception as e:
                logger.error(f"Error in model streaming: {str(e)}")
                error_chunk = StreamChunk(type="error", message=str(e))
                token_queue.put_nowait(error_chunk)
                end_chunk = StreamChunk(type="end")
                token_queue.put_nowait(end_chunk)
        
        # Start the streaming task
        stream_task = asyncio.create_task(stream_model())
        
        # Process tokens from queue
        accumulated_content = ""
        while True:
            try:
                # Check if client disconnected
                if request and await request.is_disconnected():
                    logger.info("Client disconnected, stopping stream")
                    handler.cancel()
                    stream_task.cancel()
                    break
                
                # Wait for next chunk with timeout
                try:
                    chunk = await asyncio.wait_for(token_queue.get(), timeout=0.1)
                except asyncio.TimeoutError:
                    continue
                
                if chunk.type == "token" and chunk.content:
                    accumulated_content += chunk.content
                    token_data = {"event": "token", "content": chunk.content}
                    yield format_sse(token_data)
                
                elif chunk.type == "usage" and chunk.usage:
                    usage_data = {"event": "usage", "usage": chunk.usage}
                    yield format_sse(usage_data)
                
                elif chunk.type == "error":
                    error_data = {"event": "error", "message": chunk.message}
                    yield format_sse(error_data)
                    break
                
                elif chunk.type == "end":
                    # Stream completed successfully, persist history
                    if accumulated_content:
                        try:
                            # Add assistant message to history
                            ai_message = AIMessage(content=accumulated_content)
                            await asyncio.to_thread(history.add_message, ai_message)
                            logger.info(f"Persisted assistant message to session {session_id}")
                        except Exception as e:
                            logger.error(f"Failed to persist history: {str(e)}")
                    
                    end_data = {"event": "end"}
                    yield format_sse(end_data)
                    break
                    
            except Exception as e:
                logger.error(f"Error processing stream chunk: {str(e)}")
                error_data = {"event": "error", "message": "Stream processing error"}
                yield format_sse(error_data)
                break
        
        # Ensure streaming task is cancelled
        if not stream_task.done():
            stream_task.cancel()
            try:
                await stream_task
            except asyncio.CancelledError:
                pass
                
    except Exception as e:
        logger.error(f"Error in conversation streaming: {str(e)}")
        error_data = {"event": "error", "message": str(e)}
        yield format_sse(error_data)
    
    finally:
        logger.info(f"Conversation stream ended for session {session_id}")


async def stream_explanation(
    question: str,
    model_name: Optional[str] = None,
    request: Optional[Request] = None,
    **model_params
) -> AsyncGenerator[bytes, None]:
    """
    Stream an explanation response using Server-Sent Events.
    
    Args:
        question: Question to explain
        model_name: Optional model override
        request: FastAPI request object for disconnect detection
        **model_params: Additional model parameters
        
    Yields:
        SSE-formatted bytes for each streaming event
    """
    logger.info(f"Starting explanation stream for question: {question[:100]}...")
    
    # Create bounded queue for token forwarding
    token_queue: asyncio.Queue = asyncio.Queue(maxsize=100)
    handler = StreamingForwarderHandler(token_queue)
    
    try:
        # Get model factory and create streaming model
        factory = get_model_factory()
        
        # Create model with streaming enabled and callback handler
        model = factory.create_model(
            model_name or "explain", 
            streaming=True,
            callbacks=[handler],
            **model_params
        )
        
        if not model:
            error_data = {"event": "error", "message": "Failed to create model"}
            yield format_sse(error_data)
            return
        
        # Build prompt for explanation
        prompt_template = get_chat_prompt_template("explain")
        messages = []
        
        if prompt_template:
            formatted_messages = prompt_template.format_messages(question=question)
            messages.extend(formatted_messages)
        else:
            # Fallback if no prompt template
            from langchain_core.messages import HumanMessage
            messages.append(HumanMessage(content=f"Please explain: {question}"))
        
        # Emit start event
        start_data = {
            "event": "start", 
            "model": model_name or "explain"
        }
        yield format_sse(start_data)
        
        # Start streaming in background task
        async def stream_model():
            try:
                async for chunk in model.astream(messages):
                    pass  # Tokens are handled by the callback
            except Exception as e:
                logger.error(f"Error in model streaming: {str(e)}")
                error_chunk = StreamChunk(type="error", message=str(e))
                token_queue.put_nowait(error_chunk)
                end_chunk = StreamChunk(type="end")
                token_queue.put_nowait(end_chunk)
        
        # Start the streaming task
        stream_task = asyncio.create_task(stream_model())
        
        # Process tokens from queue
        while True:
            try:
                # Check if client disconnected
                if request and await request.is_disconnected():
                    logger.info("Client disconnected, stopping stream")
                    handler.cancel()
                    stream_task.cancel()
                    break
                
                # Wait for next chunk with timeout
                try:
                    chunk = await asyncio.wait_for(token_queue.get(), timeout=0.1)
                except asyncio.TimeoutError:
                    continue
                
                if chunk.type == "token" and chunk.content:
                    token_data = {"event": "token", "content": chunk.content}
                    yield format_sse(token_data)
                
                elif chunk.type == "usage" and chunk.usage:
                    usage_data = {"event": "usage", "usage": chunk.usage}
                    yield format_sse(usage_data)
                
                elif chunk.type == "error":
                    error_data = {"event": "error", "message": chunk.message}
                    yield format_sse(error_data)
                    break
                
                elif chunk.type == "end":
                    end_data = {"event": "end"}
                    yield format_sse(end_data)
                    break
                    
            except Exception as e:
                logger.error(f"Error processing stream chunk: {str(e)}")
                error_data = {"event": "error", "message": "Stream processing error"}
                yield format_sse(error_data)
                break
        
        # Ensure streaming task is cancelled
        if not stream_task.done():
            stream_task.cancel()
            try:
                await stream_task
            except asyncio.CancelledError:
                pass
                
    except Exception as e:
        logger.error(f"Error in explanation streaming: {str(e)}")
        error_data = {"event": "error", "message": str(e)}
        yield format_sse(error_data)
    
    finally:
        logger.info("Explanation stream ended")